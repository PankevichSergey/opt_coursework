\documentclass[a4paper,12pt]{extarticle}
\usepackage{geometry}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english,russian]{babel}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{fancyhdr}
\usepackage{setspace}
\usepackage{graphicx}
\usepackage{colortbl}
\usepackage{tikz}
\usepackage{pgf}
\usepackage{subcaption}
\usepackage{listings}
\usepackage{indentfirst}
\usepackage{algorithm}

\newcommand{\argmin}{\mathop{\rm argmin}}
\usepackage{algorithmic}
\usepackage[
backend=biber,
style=numeric,
maxbibnames=99
]{biblatex}
\addbibresource{refs.bib}
\usepackage[colorlinks,citecolor=blue,linkcolor=blue,bookmarks=false,hypertexnames=true, urlcolor=blue]{hyperref} 
\usepackage{indentfirst}
\usepackage{mathtools}
\usepackage{booktabs}
\usepackage[flushleft]{threeparttable}
\usepackage{tablefootnote}

\usepackage{chngcntr} % нумерация графиков и таблиц по секциям
\counterwithin{table}{section}
\counterwithin{figure}{section}

\graphicspath{{graphics/}}%путь к рисункам

\makeatletter
% \renewcommand{\@biblabel}[1]{#1.} % Заменяем библиографию с квадратных скобок на точку:
\makeatother

\geometry{left=2.5cm}% левое поле
\geometry{right=1.0cm}% правое поле
\geometry{top=2.0cm}% верхнее поле
\geometry{bottom=2.0cm}% нижнее поле
\setlength{\parindent}{1.25cm}
\renewcommand{\baselinestretch}{1.5} % междустрочный интервал

\newcommand{\eqdef}{\coloneqq}
\def\<#1,#2>{\langle #1,#2\rangle}
\newcommand{\norm}[1]{\|#1\|_{2}}
\newcommand{\sqn}[1]{\norm{#1}^2}
\newcommand{\bibref}[3]{\hyperlink{#1}{#2 (#3)}} % biblabel, authors, year
\addto\captionsrussian{\def\refname{Список литературы (или источников)}} 

\renewcommand{\theenumi}{\arabic{enumi}}% Меняем везде перечисления на цифра.цифра
\renewcommand{\labelenumi}{\arabic{enumi}}% Меняем везде перечисления на цифра.цифра
\renewcommand{\theenumii}{.\arabic{enumii}}% Меняем везде перечисления на цифра.цифра
\renewcommand{\labelenumii}{\arabic{enumi}.\arabic{enumii}.}% Меняем везде перечисления на цифра.цифра
\renewcommand{\theenumiii}{.\arabic{enumiii}}% Меняем везде перечисления на цифра.цифра
\renewcommand{\labelenumiii}{\arabic{enumi}.\arabic{enumii}.\arabic{enumiii}.}% Меняем везде перечисления на цифра.цифра

\begin{document}
\input{title_kr}% это титульный лист - выберите подходящий вам из имеющихся в проекте вариантов (kr - курсовая работа у 3 курса, vkr - выпускная квалификационная работа у 4 курса)
\newpage
\setcounter{page}{2}

{   

	\hypersetup{linkcolor=black}
	\tableofcontents
}

\newpage

\newpage
\section*{Аннотация}   % this is how to use russian

\addcontentsline{toc}{section}{Аннотация}
Алгоритмы распределенной оптимизации становятся все более актуальными,
так как современные модели машинного обучения зачастую тренируются на массивах данных,
которые невозможно разместить на одном вычислительном устройстве в силу их большого размера или приватности.
В данной работе рассматривается случай централизованной оптимизации, где один сервер хранит параметры модели,
данные разделены между несколькими устройствами и сервером, управляющим процессом оптимизации.
Целью работы является получение лучшего на практике алгоритма
для случая сильно выпуклых функций и в условиях схожести слагаемых. 
\section*{Ключевые слова}
Методы оптимизации, распределенная оптимизация, централизованная оптимизация, сильная выпуклость, схожесть слагаемых.
\pagebreak


% chktex-file 8
% chktex-file 3
% chktex-file 25

\section{Введение}

\subsection{Описание предметной области}
\newcommand{\zij}{z_{i}^{(j)}}
\newcommand{\lxz}{l (x, \zij)}
\newcommand{\fj}{\sum_{i=1}^{n} \lxz}
\newcommand{\R}{\mathbb{R}}

Распределенная оптимизация возникает в случае, когда функционал зависит от данных, размещенных на различных носителях.
Например, в случае поиска параметров некой модели машинного обучения оптимизируется эмпирическая функция потерь.
В централизованном случае распределенной оптимизации данные размещены на сервере и других устройствах.
Происходит итерационный процесс коммуникаций, в ходе которого сервер поручает другим устройствам выполнять некоторые вычисления,
они направляют результаты обратно серверу и сервер выполняет шаги оптимизационного процесса, используя данные,
размещенные на нем и полученные значения от других устройств.


\subsection{Постановка задачи}
В данной работе работе рассматривается централизованная оптимизация в случае, когда слагаемые,
из которых состоит оптимизируемый функционал, в какой-то мере схожи между собой,
a сам функционал является сильно выпуклым.
Целью работы является исследование возможностей улучшения алгоритмов, работающих при упомянутых
ограничениях, для лучшего применения на практике. Кроме практической части в данной работе 
так же приводятся теоретическое исследование полученного метода.

\section{Формальная постановка задачи}

Скажем, что всего есть $n$ устройств и $m$ элементов обучающей выборки(примеров) на каждом устройстве, $N = nm$ -- суммарное количество примеров.
Обозначим за $z_i^{(j)}$ -- элемент обучающей выборки под номером $i$ на устройстве номер $j$. Будем обозначать за $x\in\R^d$ параметры.
Обозначим за  $l(x, z)$ -- функцию потерь на примере $z$, при векторе параметров $x$. Кроме того введем следуюшие обозначения:

\begin{equation}
    \label{eq:f_i}
         f_i(x) = \frac{1}{m} \sum_{j=1}^{m} l(x, z_i^j)
\end{equation}

\begin{equation}
    \label{eq:F}
         r(x) = \frac{1}{n} \sum_{i=1}^{n} f_i(x)
\end{equation}

Если вектор параметров равен $x$, то $r(x)$ представляет из себя эмпирический риск на всем массиве данных, 
$f_i(x)$ -- на устройстве $i$.


Нашей целью будет являться решение следующей задачи оптимизации
\begin{equation}
    \label{eq:opt_x}
         \min_{x\in\R^d}~~ r(x) = \frac{1}{n} \sum_{i=1}^{n} f_i(x)
\end{equation}


В [] предлагается представить функцию в следующем виде: 
\begin{equation}
    \label{eq:rpq}
         r(x) = p(x) + q(x), 
\end{equation}
\begin{equation}
    \label{eq:q}
         q(x) = f_1(x)
\end{equation}
\begin{equation}
    \label{eq:p}
         p(x) = \frac{1}{n}\sum_{i = 1}^{n} \left[f_i(x) - f_1(x)\right].
\end{equation}


В дальнейшем мы увидим, что это способствует уменьшению числа коммуникаций, если $f_i$ похожи между собой.


\subsubsection*{Предположения}
Функция $g:\R^d\to\R$ называется $L-$гладкой на множестве $\mathbb{R}^d$, если ее градиент $L-$липшицев,
то есть верно
\begin{equation}
    \label{eqn:strong_convex}
    \norm{\nabla g(y) - \nabla g(x)} \leq L \norm{x - y}, \quad \forall\, x, y \in \mathbb{R}^d.
\end{equation}



Функция $g:\R^d\to\R$ называется $\mu-$сильно выпуклой на множестве $\mathbb{R}^d$,
если для положительного $\mu$ выполняется
\begin{equation}
    \label{eqn:strong_convex}
    g(y) \geq g(x) + \nabla g(x)^\top(y - x) + \frac{\mu}{2}\sqn{x - y}, \quad \forall\, x, y \in \mathbb{R}^d.
\end{equation}

Теоретический анализ и все методы построены в следующих предположениях: 

\begin{itemize}
    \item $p(x)$ является $L_p-$гладкой на $\mathbb{R}^d$,
    \item $q(x)$ является $L_q-$гладкой и выпуклой на $\mathbb{R}^d$, 
    \item $r(x)$ является $L-$гладкой и $\mu$-сильно выпуклой на $\mathbb{R}^d$.
\end{itemize}

Отметим, что последнее предположение часто выполняется в задачах машинного обучения из-за добавления
$L_2-$регуляризации к выпуклым функциям потерь, и чем больше коэффициент регуляризации, тем выше константа
сильной выпуклости. На практике оптимальный коэффициент регуляризации обычно довольно мал, из-за 
чего классические методы с оценкой числа итераций до достижения определенной точности
пропорциональной $k$(алгоритм [??]) или $\sqrt{k}$(алгоритм [??]) (где $k = \frac{L}{\mu}$ ~-- число обусловленности),
сходятся заметно медленее методов, разработанных для случаев, когда имеет место схожесть слагаемых(алгоритмы [??], [??]).


%[?? [https://en.wikipedia.org/wiki/Convex_function#cite_note-nesterov-14]]


\section{Методы решения задачи}
\subsection{Примитивы}
Для начала определим примитив, которым будут пользоваться последующие алгоритмы

    \begin{algorithm}[]
    \caption{Подсчет $\nabla p(x)$}
    \begin{algorithmic}[1]
    
    \STATE{} \textbf{Input}: $x \in \mathbb{R}^d$
    \STATE{} \textbf{send} $x$ to all computers 
    \vspace{0.5ex}
    \vspace{0.5ex}
    \FOR{$i=2,3\ldots, n$} 
    \vspace{0.5ex}
    \STATE{} \textbf{receive} $x$
    \vspace{0.5ex}
    \STATE{} \textbf{compute} $\nabla f_i(x)$
    \vspace{0.5ex}
    \STATE{} \textbf{send} $\nabla f_i(x)$
    \vspace{0.5ex}
    \ENDFOR{}
    \vspace{0.5ex}
    \STATE{} \textbf{receive} $\nabla f_2(x), \nabla f_2(x), \ldots, \nabla f_n(x)$
    \STATE{} \textbf{compute} $\nabla f_1(x)$
    \STATE{} \textbf{compute} $\nabla p(x) = \frac1n \sum_{i = 1}^{n} \left[\nabla f_i(x)  - \nabla f_1(x) \right]$
    \vspace{0.5ex}
    \RETURN{} $\nabla p(x)$
    \end{algorithmic}
    \end{algorithm}

    Приведем подробное описание схемы взаимодействия сервера и других устройств,
    Сервер получает на вход значение $x$. В строчке 2 он отсылает его всем устройствам. 
    После чего все устройства параллельно исполняют строчки 4, 5 и 6.
    В строчке 8 сервер получает резульаты вычислений всех устройств. 
    В конце он вычисляет $\nabla r(x)$, используя полученные значения градиентов. 


    Данное действие требует 1 раунд коммуникаций и производит один вызов оракула градиента на сервере. 


    \begin{algorithm}[]
    \caption{Подсчет $\nabla q(x)$}
    \begin{algorithmic}[1]
    \STATE{} \textbf{Input}: $x \in \mathbb{R}^d$
    \STATE{} \textbf{compute} $\nabla f_1(x)$
    \RETURN{} $\nabla f_1(x)$
    \end{algorithmic}
    \end{algorithm}

    Вычисление $\nabla q(x)$ не требует коммуникаций и производит один вызов оракула градиента на сервере. 

    \begin{algorithm}[]
    \caption{Подсчет $\nabla r(x)$}
    \begin{algorithmic}[1]
    \STATE{} \textbf{Input}: $x \in \mathbb{R}^d$
    \STATE{} \textbf{compute} $\nabla q(x)$
    \STATE{} \textbf{compute} $\nabla p(x)$
    \RETURN{} $\nabla q(x) + \nabla p(x)$
    \end{algorithmic}
    \end{algorithm}

\subsection{Централизованная версия градиентного спуска}

Приведем пример работы градиентного спуска в условиях централизованной оптимизации.

\begin{algorithm}[]
    \caption{Централизованный градиентный спуск}
    \begin{algorithmic}[1]

    \STATE{} \textbf{Input}: $x^0 \in \mathbb{R}^d$
    \STATE{} \textbf{Parameters}: $\eta, K$
    \FOR{$t=0,1\ldots, K - 1$} 
    \STATE{} $x_{t + 1} = x_t - \eta \nabla r(x_t)$
    \ENDFOR{}
    \RETURN{} $x_{K}$
    \end{algorithmic}
\end{algorithm}

Где 
$$ \textstyle \eta = \frac{1}{L}.$$

\subsection{Централизованная версия ускоренного градиентного спуска}

\begin{algorithm}[]
    \caption{Централизованный ускоренный градиентный спуск}
    \begin{algorithmic}[1]

    \STATE{} \textbf{Input}: $x^0 \in \mathbb{R}^d$
    \STATE{} \textbf{Parameters}: $\eta, \kappa, \mu, L, \gamma$
    \STATE{} $y_0 = x_0$
    \FOR{$t=0,1\ldots, K - 1$} 
    \STATE{} $y_{t + 1} = x_{t} - \eta \nabla r(x_t)$
    \STATE{} $x_{t + 1} = (1 + \gamma)y_{t+1} - \gamma y_{t}$
    \ENDFOR{}
    \RETURN{} $x_{K}$
    \end{algorithmic}
\end{algorithm}

Где
$$ 
\textstyle
\eta = \frac{1}{\eta},
\quad \kappa = \frac{L}{\mu},
\quad \gamma = \frac{\sqrt{\kappa} - 1}{\sqrt{\kappa} + 1}.
$$

Данные алгоритмы эквивалентны градиентному и ускоренному градиентному спуску в
классическом нераспределенном случае по производимым вычислениям.
Мы будем использовать их в качестве базовых решений для сравнения эффективности по числу коммуникаций и
числу вызовов оракула градиента.


\subsection{Accelerated Extragradient}

В статье [???] приведен алгоритм,
который является асимптотически оптимальным как по числу локальных вызовов оракула градиента,
так и по числу коммуникаций. 


\begin{algorithm}[]
    \caption{Accelerated Extragradient}
    \begin{algorithmic}[1]

    \STATE{} \textbf{Input}: $x^0 \in \mathbb{R}^d$
    \STATE{} \textbf{Parameters}: $K, \eta, \theta, \alpha$
    \STATE{} $x_f^0 = x^0$
    \FOR{$t=0,1\ldots, K - 1$} 
    \STATE{} $x_g^k = \tau x^k + (1-\tau)x^k_f$
    \STATE{} $x_f^{k+1} \approx \argmin_{x \in \R^d}\left[ A_\theta^k(x) \eqdef p(x_g^k) + \<\nabla p(x_g^k),x - x_g^k> + \frac{1}{2\theta}\sqn{x - x_g^k} + q(x)\right]$
    \STATE{} $x^{t+1} = x^t + \eta\alpha (x_f^{t+1}  - x^t)- \eta \nabla r(x_f^{t+1})$
    \ENDFOR{}
    \RETURN{} $x^{K}$
    \end{algorithmic}
\end{algorithm}

Где $$ \textstyle
\tau = \min\left\{1,\frac{\sqrt{\mu}}{2\sqrt{L_p}}\right\},
\quad \theta = \frac{1}{2L_p},
\quad  \eta = \min \left\{\frac{1}{2\mu},\frac{1}{2\sqrt{\mu L_p}}\right\},
\quad \alpha = \mu;
$$
В данном случае в строчке 6 имеется в виду приближенное решение задачи, для которого соблюдается условие 
\[	\sqn{\nabla A_\theta^k(x_f^{k+1})} \leq  \frac{L_p^2}{3}\sqn{x_g^k- \argmin_{x \in \R^d} A_\theta^k(x)}\]


Данный алгоритм на каждой итерации совершает 2 раунда коммуникации.
Один для вычисления $\nabla r(x_f^{t + 1})$ и один во время вычисления $\nabla p(x_g^k)$  
в начале решения подзадачи. Кроме того при решении подзадачи алгоритм $T$ раз вычисляет $\nabla q$. В итоге алгоритм совершает \textbf{$2K$} вызовов \textbf{$\nabla p$} и $(T + 1)K$ вызовов $\nabla q$,
если вычислять $\nabla r(x)$ как  $\nabla p(x) + \nabla q(x)$.


В статье [] показано, что при 
	\begin{equation}\label{eq:K}
	\textstyle	K\geq 2\max \left\{1, \sqrt{\frac{L_p}{\mu}}\right\}\log \frac{\sqn{x^0 - x^*}
		+\frac{2\eta}{\tau}\left[R(x^0) - R(x^*)\right]}{\varepsilon},
	\end{equation}
	Расстояние до решения задачи $x^*$:
	\begin{equation}\label{eq:accuracy}
	\textstyle	\sqn{x^K - x^*} \leq \varepsilon.
	\end{equation}

И условие [] гарантированно будет выполнено следующем числе итераций решения подзадачи
\begin{equation}\label{eq:T}
    \textstyle T = \sqrt[4]{3}D\max\left\{1, \sqrt{\frac{L_q}{L_p}}\right\},
\end{equation}

где $D$ является констаной, не зависящей от констант выпуклости и гладкости.


Что дает следуюшие оценки при условии $\mu \leq L_p \leq L_q$:
    \begin{equation*}
    \textstyle \mathcal{O} \left(\sqrt{\frac{L_q}{\mu}}\log\frac{1}{\epsilon}\right) ~~ \text{вызовов}\,\, \nabla q(x)~~{\text{;} }\quad 
    \mathcal{O}  \left(\sqrt{\frac{L_p}{\mu}}\log\frac{1}{\epsilon}\right) ~~ \text{вызовов}\,\, \nabla p(x).
    \end{equation*}

И это является асимптотически оптимальным результатом.


\subsubsection{Алгоритм решения}

\subsection{Practical Accelerated Extragradient}
Заметим, что условие [???] непрактично, так как невозможно применять итерационный метод 
для поиска решения и прекращать итерации, как только оно стало выполнено. Это из-за того 
что в выражении справа необходимо знать $\argmin_{x \in \R^d} A_\theta^k(x)$. Поэтому 
в представленном в статье [???] алгоритме необходимо делать фиксированное число итераций $T$ 
оптмизационного процесса такое, что условие [] гарантированно выполнено. 

В нашем методе будем искать решение такое, что выполняется другое условие:
\[	\sqn{\nabla A_\theta^k(x_f^{k+1})} \leq  L_p^2 \sqn{x_g^k- x_f^{k+1}} \]
Искать эту точку будем, минимизируя $A_{\theta}^k$, с использованием ускоренного градиентного спуска,
начиная из точки $x_g$.


\begin{algorithm}[]
    \caption{Subproblem solver 2}
    \begin{algorithmic}[1]

    \STATE{} \textbf{Input}: $x_g^k \in \mathbb{R}^d$
    \STATE{} \textbf{Parameters}: $\eta, \kappa, \mu, L_p, \gamma$
    \STATE{} $y_0 = x_0 = x_g^k$
    \STATE{} compute $\nabla p(x_g^k)$
    \FOR{$t=0,1\ldots$} 
    \STATE{} compute $\nabla q(x_t)$
    \STATE{} $\nabla A_{\theta}^k(x_t) = \nabla p(x_g^k) + \frac{1}{\theta}(x_t - x_g^k) + \nabla q(x_t)$
    \STATE{} \textbf{if} $\nabla A_{\theta}^k(x_t) \leq L_p^2 \sqn{x_g^k- x_f^{k+1}}$ 
    \RETURN{} $x_{t}$
    \vspace{2ex}
    \STATE{} $y_{t + 1} = x_{t} - \eta \nabla r(x_t)$
    \STATE{} $x_{t + 1} = (1 + \gamma)y_{t+1} - \gamma y_{t}$
    \vspace{0.5ex}
    \ENDFOR{}
    \end{algorithmic}
\end{algorithm}


Данный алгоритм в отличие от предыдущего позволяет завершать процесс раньше,
чем будет произведено количество итераций, соответсвующее верхней оценке,
приведенной в последующем анализе. В части с экспериментами на реальных и синтетических
данных показано, что это позволяет добиться заметно меньшего числа вызовов
оракула градиента. 

\section{Теоретический анализ}




Отметим несколько свойств функции $A_\theta^k(x)$,
которые важны для приближенного решения задачи минимизации:

\begin{itemize}
    \item  $\nabla A_{\theta}^k(x)$ имеет константу липшицевости $2L_p + L_q$
    
    Доказательство:

    
    $\nabla A_\theta^k(x) = \nabla p(x_g^k) + \frac{1}{\theta}(x - x_g^k) + \nabla q(x)$
    
    Первое слагаемое константно, а следующие два дают,
    что градиент липшицев с константой  $\frac{1}{\theta} + L_q = 2L_p + L_q$ 

    \item $A_{\theta}^k(x)$ является $\frac{1}{\theta}$-сильно выпуклой
    
    Доказательство: 

    $p(x_g^k)$ константна, $\<\nabla p(x_g^k),x - x_g^k>$ линейна,
    $q(x)$ выпукла, поэтому слагаемое $\frac{1}{2\theta}\sqn{x - x_g^k}$ обеспечивает, 
    что $A_{\theta}^k(x)$ является $\frac{1}{\theta} = 2L_p$-сильно выпуклой
\end{itemize}

\subsection{Число итераций для решения поздадачи}

Необходимо найти такое $x_f^{k + 1}$,
для которого верно $\sqn{\nabla A_{\theta}^k(x_f^{k+1})} \leq L_p^2 \sqn{x_g^k- x_f^{k+1}}$,
что эквивалентно $\norm{\nabla A_{\theta}^k(x_f^{k+1})} \leq L_p \norm{x_g^k- x_f^{k+1}}$

Из липшицевости градиента следует 
\[\norm{\nabla A_{\theta}^k(x)} \leq (2L_p + L_q)\norm{x - x_*}\]

Значит достаточно, чтобы выполнялось 

\[\norm{x_f^{k + 1} - x_*} \leq \beta \norm{x_g^k- x_f^{k+1}}\]

Где $\beta = \frac{L_p}{2L_p + L_q}$


Воспользуемся неравенством треугольника и получим
\[\norm{x_f - x_g} \geq \norm {x_g - x_*} - \norm{x_f - x_*}
\]


Тогда достаточно 

\[\norm{x_f^{k + 1} - x_*} \leq \beta (\norm {x_g - x_*} - \norm{x_f - x_*})\]

\[\norm{x_f^{k + 1} - x_*} \leq \frac{\beta}{1 + \beta} \norm{x_g^k- x_*}\]

Что эквивалентно

\[\frac{\norm{x_f^{k + 1} - x_*}}{\norm{x_g^k- x_*}} \leq \frac{\beta}{1 + \beta} \]

То есть ошибка по аргументу минимизируемой функции должна составлять $\frac{\beta}{1 + \beta}$ от изначальной. 
Для удобства анализа привведем во сколько раз должна уменьшиться невязка,
чтобы данное условие гарантированно выполнилось.

Для этого необходимо оценить сверху отношение невязок,
если ошибки по аргументу отличаются в $\frac{\beta}{1 + \beta}$ раз. 


Нам понадобятся два неравенства cледующих неравенства:


\[A_{\theta}^k(x) - A_{\theta}^k(x_*) \geq
\nabla A_{\theta}^k(x_*)^T(x-x_*) + \frac{1}{2\theta}\sqn{x-x_*} = 
\frac{1}{2\theta}\sqn{x-x_*}; \]




\[A_{\theta}^k(x) - A_{\theta}^k(x_*) \leq \int_{0}^{\norm{x-x_*}} (2L_p + L_q) t dt =
(2L_p + L_q) \frac{\sqn{x-x_*}}{2}; \]


первое следует из сильной выпуклости, а второе из липшицевости градиента. 

Таким образом $A_{\theta}^k(x_f^{k + 1}) - A_{\theta}^k(x_*)  \geq \frac{1}{2\theta}\sqn{x_f^{k + 1}-x_*}$
и $A_{\theta}^k(x_g) - A_{\theta}^k(x_*) \leq (2L_p + L_q) \frac{\sqn{x_g-x_*}}{2}$, значит

\[\frac{\sqn{x_f^{k + 1}-x_*}}{\sqn{x_g-x_*}} 
\leq \theta(2L_p + L_q) \frac{A_{\theta}^k(x_f^{k + 1}) -
 A_{\theta}^k(x_*)}{A_{\theta}^k(x_g) - A_{\theta}^k(x_*)}\]


Тогда для [???] % отношение ошибок 
достаточно \[\frac{A_{\theta}^k(x_f^{k + 1}) - A_{\theta}^k(x_*)}{A_{\theta}^k(x_g) - A_{\theta}^k(x_*)}
\leq \frac{1}{\theta(2L_p + L_q)} \frac{\beta^2}{(1 + \beta)^2} =
\frac{2\beta^3}{(1 + \beta)^2} \]

Так как $\beta \leq 1$, неравенство выше выполняется и когда


\[\frac{A_{\theta}^k(x_f^{k + 1}) - A_{\theta}^k(x_*)}{A_{\theta}^k(x_g) - A_{\theta}^k(x_*)}
\leq \frac{\beta^3}{2}\]



Найдем число обусловленности $A_\theta^k$: \[\kappa_A = \frac{2L_p + L_q}{\frac{1}{\theta}} = \frac{2L_p + L_q}{2L_p} = \frac{1}{2\beta}\]


Тогда для достижения [???] ускоренному градиентному спуску необходимо 
\[O\left(\sqrt{\kappa_A} \log \frac{2}{\beta^3}\right) = 
O\left(\sqrt{\kappa_A}  \log\frac{1}{\beta}\right) = 
O\left(\sqrt{\kappa_A}  \log \kappa_A\right)\]


итераций

Заметим, что в предположении $\mu \leq L_q \leq L_p$







\section{Условие схожести слагаемых}
Скажем, что функции $f(x)$ и $g(x)$ являются $\delta$-cхожими на некотором множестве $S$, если выполнено следующее. 
\begin{equation}\label{eqn:Hessian-approx-mu}
   \left\|\nabla^2 f(x) - \nabla^2 g(x)\right\| \leq \delta, 
   \quad \forall\, x\in S,
\end{equation}
Рассматриваемая нами функция $r$ имеет вид суммы нескольких слагаемых.
Заметим, что если данные на каждом устройстве получены независимо из одного
и того же распределения, то $f_i$ весьма вероятно похожи между собой и хорошо приближают функцию $r$.
А именно, в (Troop J., 2015)\cite{Troop-paper} с помощью матричного неравенства Хеффдинга показано,
что с вероятностью не меньше $1-p$ выполнено:

\begin{equation}\label{eqn:matrix-hoeffding}
   \biggl\|\nabla^2f_i(x)\!-\!\nabla^2 r(x)\biggr\|
   \!\leq\! \sqrt{\frac{32 A_l^2 \log(d/p)}{m}},
\end{equation}
Для некоторого числа $A_l$ такого, что $A_l \geq \|\nabla^2\ell(x,z_i)\| \ \forall\, x\in S$.


Упрощенно это можно записать так: с высокой вероятностью $\delta=\tilde{O}(1/\sqrt{m})$, где 
$\tilde{O}$ показывает асимпотическую зависимость без учета размерности и логарифмических множителей.


Пусть выполнена $\delta$-cхожесть слагаемых: 
\begin{equation}
    \norm{\nabla^2 f_i(x) - \nabla^2 f_1(x)} \leq \delta, \ \forall i \in \{ 1\ldots n\},\ x\in R^d.
\end{equation}

Тогда

\[\nabla^2 p(x) = \frac1n \sum_{i = 1}^{n} \left[\nabla^2 f_i(x) - \nabla^2 f_1(x) \right]\]
\[L_p \leq \norm{\nabla^2 p(x)} \leq \frac{1}{n} \sum_{i =1}^{n} \delta  = \delta.\]

Во многих исследованиях $\delta$-cхожесть используется для уменьшения числа коммуникаций
[??] [??Hendrikx]. В алгоритмах [] и [] для ее использования 
функция $r(x)$ разделялась на $q(x)$, вся информация о которой содержится на сервере 
и не требует коммуникаций, и на $p(x)$, для которой выполнено $L_p \leq \delta=\tilde{O}(1/\sqrt{m})$. 
 
\section{Экспериментальное исследование метода}
    Будем рассматривать следующую задачу минимизации эмпирического риска (Ridge-регрессию)
    \[ F(w) = \frac{\lambda}{2} ||w||_2^2 + \frac{1}{2N} \sum_{i=1}^n (w^t x_i - y_i)^2 \to_{w} \min \]
     Где $x_i$ -- вектор параметров объекта $i$, $w$ -- параметры модели, $ \lambda $ -- коэффициент регуляризации.
    Эквивалентно ее можно переписать в матричном виде следующим образом: 
        \[\frac{1}{2N}||Xw - y||_2^2 + \frac{\lambda}{2} ||w||_2^2 \to_{w} \min \]
    Где $X$ -- матрица высоты $N$ и ширины $d$, в строке $i$ которой находится $x_i^t$.
    Будем ее называть матрицей дизайна. 
   
    Будем делать два типа экспериментов: на синтетических данных и реальных.
    В обоих случаях распределенность будет симулироваться.
    Мы будем производить все вычисления на одном устройстве, но при соверешении действий,
    эквивалентных коммуникации сервера и устройств, будем инкрементировать счетчик числа коммуникаций.
    Аналогично при подсчете градиента функции, относящихся к слагаемым на одной из устройств,
    будем инкрементировать счетчик числа локальных вызовов. 

    \subsection{Эксперимент на синтетических данных}
    Будем рассматривать сеть из $n$ устройств, первая из которых является сервером.
    На каждом устройстве будут располагаться $m$ примеров.
        
    \begin{itemize}
    \item  Сгенерируем матрицу дизайна для сервера $X_1$,
    где $X_{1ij} \sim^{i.i.d} \mathcal{N}(0, L_{mult})$ c использованием  \textbf{numpy}.
    Константа $L_{mult}$ позволяет регулировать константу липшица градиента функции потерь по параметрам.
    Чем больше $L_{mult}$, тем в среднем выше константа липшицевости. 
    
    \item После чего сгенерируем вектор $w$, $ w_i \sim^{i.i.d} \mathcal{N}(0, 1)$. 
    
    \item Теперь положим $y = X_1w + \varepsilon$, где $\varepsilon_i \sim^{i.i.d} \mathcal{N}(0, 1)$
    \newcommand\tab[1][1cm]{\hspace*{#1}}
    

    \tab{} Это соответствует классическим статистическим предположениям в задаче линейной регрессии. 

    \item Сгенерируем матрицы дизайна для выборки других устройств ($k \geq 2$) следующим образом $X_k = X_1 + \delta_{mult} P_k$,
     где $P_{kij} \sim^{i.i.d} \mathcal{N}(0, 1)$.
    Константа $\delta_{mult}$ позволяет регулировать схожесть данных на сервере и других устройствах.
    Чем она меньше, тем в среднем ближе будут гессианы функций на разных устройствах. 
    
    \end{itemize}

    Частично для приближения к условиям статьи [] были выбраны следующие константы: 
    $$ \textstyle
    \lambda = 0.1,
    \quad n = 25,
    \quad m = 100,
    \quad d = 50,
    \quad \delta_{mult} = 0.1,
    \quad L_{mult} = 2.
    $$
    \subsection{Эксперимент на реальных данных}
    В качестве реальных данных для эксперимента использовался
    \href{https://www.kaggle.com/datasets/budincsevity/szeged-weather/data}{набор данных},
    в котором собрана информация о погоде. 
 
    В качестве признаков выбирались столбцы 
    \begin{itemize}
        \item ``Wind Speed (km/h)'
        \item ``Humidity'
        \item ``Wind Bearing (degrees)'
        \item ``Visibility (km)'
        \item ``Loud Cover'
        \item ``Pressure (millibars)'
        \item ``Temperature (C)'
    \end{itemize}
    В качестве предсказываемой величины ``Apparent Temperature (C)'.

    
    Кроме того к признакам добавлен единичный свободный член.
    Признаки отнормированы так, что имеют нулевое среднее и единичную выборочную дисперсию.
    Объекты распределены между 25 устройствами в случайном порядке. 

    
    \subsection{Оценка констант $\delta$, $L_p$, $L_q$, $\mu$}

    Для работы некоторых методов необходимо знание констант гладкости и сильной выпуклости.
    Кроме того их знание полезно для отслеживания зависимостей качества различных методов от свойств данных. 
     \[ f_i(w) = \frac{1}{m}||X_i w - y||_2^2 + \frac{\lambda}{2} ||w||_2^2\]
    \[ \nabla_{w} f_i = \frac{1}{m}X_i^T(X_i w - y) + \lambda w\]
    \[\nabla_{w}^2 f_i  = \frac{1}{m}X_i^T X_i + \lambda I_d\]

    \subsubsection{Оценка $L_q$}
   
    \[q(w) = f_1(w) \]
    \[\nabla_{w}^2 q =  \frac{1}{m}X_i^T X_i + \lambda I_d\]
    \[L_q =  \frac{1}{m} \lambda_{\max}(X_i^T X_i ) + \lambda\]

    \subsubsection{Оценка $L_p$}
    \[p(w) = \frac{1}{n} \sum_{i=1}^{n} \left[f_i(w) - f_1(w)\right]\]
    \[\nabla_{w}^2p = \frac{1}{n}\sum_{i = 1}^{n} [\nabla_{w}^2f_i - \nabla_{w}^2f_1] = \]
    \[ = \frac{1}{n}\sum_{i = 1}^{n} [\frac{1}{m}X_i^T X_i + \lambda I_d - \frac{1}{m}X_1^T X_1- \lambda I_d] = \] 
    
    \[=\frac{1}{n}\sum_{i = 1}^{n} [\frac{1}{m}X_i^T X_i]- \frac{1}{m} X_1^T X_1 = \]

    \[L_p = \lambda_{\max}(\frac{1}{n}\sum_{i = 1}^{n} [\frac{1}{m}X_i^T X_i]- \frac{1}{m} X_1^T X_1) \]

    Где $\lambda_{\max}$ -- наибольшее собственное значение матрицы,
    которое можно найти с помощью функции \textbf{eigvalsh} из пакета \textbf{scipy},
    которая подходит для работы с симметричными вещественными матрицами. 
     

    
    \subsubsection{Оценка $\delta$}
    Отметим, что в нашем случае у всех функций гессиан является константным, поэтому 
    \[\delta = \max_{i} ||\nabla_{w}^2 f_1 - \nabla_{w}^2 f_i||_2 = \frac{1}{m} \max_{i} ||X_1^T X_1 - X_i^T X_i||\]
    
    \subsubsection{Оценка $\mu$}
    Каждая функция $\frac{1}{m}||X_iw - y||_2^2$ является выпуклой,
    $\frac{\lambda}{2} ||w||_2^2$ является $\lambda$-сильно выпуклой,
    поэтому  каждая функция $f_i$ и $r = \sum\frac{1}{n} f_i$ являются $\mu$-сильно выпуклой,
    где $\mu = \lambda$.

    \subsection{Оценка сходимости методов}
    Для оценки сходимости каждого метода на итерации $T$ будем вычислять во сколько раз квадрат расстояния до оптимума меньше,
    то есть 
    \[\frac{||w^T - w_*||_2^2}{||w^0 - w_*||},\]
    где 
    \[w_* = \argmin \frac{1}{2N}||Xw - y||_2^2 + \frac{\lambda}{2}||w||_2^2\]
    Во всех методах будем полагать $w_0 = 0$.
    Для задачи (todo) сущетсвует точное аналитическое решение:
    \[w_* = (X^T X + N\lambda I_d)^{-1}X^T y,\]
    которое мы будем использовать в экспериментах. 
    
    Отметим, что зачастую аналитическое решение непрактично из-за высокой временной сложности
    обращения матриц и вычислительной неустойчивости, поэтому на практике зачастую используются именно итерационные методы,
    подобные методам, описываемым в данной работе. Кроме того для широкого класса задач,
    не существует явного аналитического выражения решения, поэтому такие методы 

    \section{Результаты экспериментов}

    \section{Выводы}




    
\newpage 
\printbibliography[heading=bibintoc] 
	
\end{document}
